{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network example\n",
    "### Christian Igel, 2019\n",
    "\n",
    "We use TensorFlow 2.x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and visualize toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_data(N, noise = 0):\n",
    "    x = np.random.rand(N, 1) * 2 * np.pi\n",
    "    x = np.sort(x, axis=0)\n",
    "    y = np.sin(x) + np.random.normal(0, noise, (N, 1))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_sine_data(50, 0.5)\n",
    "x_val, y_val = generate_sine_data(50, 0.5)\n",
    "x_test, y_test = generate_sine_data(100, 0)\n",
    "\n",
    "print(\"Shape of training input and labels:\", x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train.reshape([-1]), y_train.reshape([-1]))\n",
    "ax.plot(x_val.reshape([-1]), y_val.reshape([-1]))\n",
    "ax.plot(x_test.reshape([-1]), y_test.reshape([-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the model in away that it is easy to create several models of the same type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='sigmoid', input_shape=(1,),\n",
    "            kernel_initializer=tf.initializers.VarianceScaling(scale=0.01**2),\n",
    "            bias_initializer=tf.initializers.TruncatedNormal(stddev=0.01)),\n",
    "        tf.keras.layers.Dense(1, activation='linear',\n",
    "            kernel_initializer=tf.initializers.VarianceScaling(scale=0.01**2),\n",
    "            bias_initializer=tf.initializers.TruncatedNormal(stddev=0.01))\n",
    "    ])\n",
    "\n",
    "model = my_model()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some operations carried out in the training loop. \n",
    "First, we  store information about the training progress in format that can be visualized by TensorBoard. Second, we store the network weights giving the lowest validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  tf.keras.callbacks.TensorBoard(log_dir='./logs/run2', update_freq='batch'),\n",
    "  #tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20),\n",
    "  tf.keras.callbacks.ModelCheckpoint('./logs/model_val.hdf5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small data sets can be fed into the model as arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization algorithm\n",
    "sgd = tf.optimizers.SGD(lr=0.2)\n",
    "\n",
    "# Compile model (i.e., build compute graph)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='MSE')\n",
    "\n",
    "# Training loop\n",
    "model.fit(x_train, y_train, batch_size=25, epochs=100, \n",
    "          validation_data=(x_val, y_val), validation_freq=1, \n",
    "          #steps_per_epoch=x_train.shape[0],\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger data sets, use the `Datset` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=16).batch(8)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "\n",
    "# Prepare the validation dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs\n",
    "!sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization algorithm\n",
    "#opt = tf.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "opt = tf.optimizers.Adam(lr=0.01)\n",
    "\n",
    "# Compile model (i.e., build compute graph)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='MSE')\n",
    "\n",
    "# Training loop\n",
    "model.fit(train_dataset, epochs=2000, verbose=1,\n",
    "          validation_data=val_dataset, validation_freq=1, \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visulaize the training and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train.reshape([-1]), y_train.reshape([-1]))\n",
    "ax.plot(x_test.reshape([-1]), y_test.reshape([-1]))\n",
    "ax.plot(x_train.reshape([-1]), pred_train.reshape([-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a network with the same structure as the one used during training\n",
    "best_model = my_model()\n",
    "# Set the weights to the weights that gave the lowest validation error during training\n",
    "best_model.load_weights('logs/model_val.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best_train = best_model.predict(x_train)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train.reshape([-1]), y_train.reshape([-1]))\n",
    "ax.plot(x_test.reshape([-1]), y_test.reshape([-1]))\n",
    "ax.plot(x_train.reshape([-1]), pred_train.reshape([-1]))\n",
    "ax.plot(x_train.reshape([-1]), pred_best_train.reshape([-1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
