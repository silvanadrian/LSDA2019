{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(n=250):\n",
    "    \"\"\" Creates a simple 1D dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    numpy.random.seed(0)\n",
    "    \n",
    "    X = numpy.linspace(0, 3*numpy.pi, n)\n",
    "    y = 0.5 * X + numpy.random.normal(0, 0.1, n)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a simple toy dataset with 10000 instances\n",
    "# and store it on disk.\n",
    "X, y = get_dataset(n=10000)\n",
    "data = numpy.concatenate([y.reshape((len(y), 1)),X.reshape((len(X), 1))], axis=1)\n",
    "numpy.savetxt('/home/lsda/artificial_regression.csv', data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLeastSquaresRegression():\n",
    "    \n",
    "    def __init__(self, lam=1.0, optimizer=\"solve\", max_iter=10, random_state=0):\n",
    "        \"\"\" Instantiates the regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lam : float, default 1.0\n",
    "            The regularization parameter lambda\n",
    "        optimizer : str, default 'solve'\n",
    "            The optimizer to be used\n",
    "        max_iter : int, default 10\n",
    "            Maximum number of iterations for optimizer\n",
    "        random_state : int, default 0\n",
    "            The random state (seed) to be used.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lam = lam\n",
    "        self.optimizer = optimizer\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Returns the parameters of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        return {\"lam\": self.lam, \n",
    "                \"optimizer\": self.optimizer,\n",
    "                \"max_iter\": self.max_iter,\n",
    "                \"random_state\": self.random_state}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        \"\"\" Sets the parameters of the model\n",
    "        \"\"\"        \n",
    "        \n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "        y : Array of shape [n_samples, 1]\n",
    "        \"\"\"   \n",
    "        \n",
    "        numpy.random.seed(self.random_state)\n",
    "        \n",
    "        # make sure that we have numpy arrays; also\n",
    "        # reshape the array X to ensure that we have\n",
    "        # a multidimensional numpy array (ndarray);\n",
    "        # for y, we want to have a column vector\n",
    "        X = numpy.array(X).reshape((X.shape[0], -1))\n",
    "        y = numpy.array(y).reshape((len(y), 1))\n",
    "\n",
    "        self._Xtrain = X\n",
    "        self._ytrain = y\n",
    "        \n",
    "        if self.optimizer == \"solve\":    \n",
    "            \n",
    "            G = numpy.dot(self._Xtrain.T, self._Xtrain) + len(self._ytrain) * self.lam * numpy.identity(self._Xtrain.shape[1])\n",
    "            B = numpy.dot(self._Xtrain.T, self._ytrain)            \n",
    "            \n",
    "            self._w = numpy.linalg.solve(G, B)\n",
    "            \n",
    "        elif self.optimizer == \"gradient\":\n",
    "            \n",
    "            from scipy.optimize import fmin_bfgs\n",
    "            \n",
    "            # initial guess\n",
    "            w0 = numpy.zeros(X.shape[1], dtype=numpy.float64)\n",
    "            \n",
    "            # additional parameters for the _function and _function_prime\n",
    "            args = (self._Xtrain, self._ytrain, self.lam)\n",
    "\n",
    "            # Note that you do not necessarily have to provide fprime function, see\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html\n",
    "            # (in this case, the gradient will be approximated via the function, as explrained\n",
    "            # here: https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)            \n",
    "            self._w = fmin_bfgs(self._function, w0, args=args, maxiter=self.max_iter)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            raise Exception(\"Unknown optimizer: {}\".format(self.optimizer))\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _function(self, w, X, y, lam):\n",
    "        \n",
    "        w = w.reshape((len(w),1))\n",
    "        diff = y - numpy.dot(X, w)\n",
    "        f = (1.0 / len(y)) * numpy.dot(diff.T, diff) + lam * numpy.dot(w.T, w)\n",
    "        \n",
    "        return f\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Computes predictions for a new set of points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "        \"\"\"           \n",
    "        \n",
    "        # make sure that we have numpy arrays; also\n",
    "        # reshape the array X to ensure that we have\n",
    "        # a multidimensional numpy array (ndarray)\n",
    "        X = numpy.array(X).reshape((X.shape[0], -1))\n",
    "            \n",
    "        preds = numpy.dot(X, self._w)\n",
    "                \n",
    "        return preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=1)\n",
    "\n",
    "lams = [10, 0.01, 0.0001]\n",
    "gammas = [0.1, 2, 1000]\n",
    "\n",
    "f, axes = plt.subplots(1, 3, sharey=True, figsize=(15,5))\n",
    "\n",
    "for (lam, gamma, i) in zip(lams, gammas, range(len(lams))):\n",
    "\n",
    "    model = LinearLeastSquaresRegression(lam=lam, optimizer=\"gradient\", max_iter=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # plot test points and predictions\n",
    "    axes[i].scatter(X_train, y_train, s=2, c=\"k\", edgecolor=\"k\", linewidths=1, label=\"Training\")\n",
    "    axes[i].scatter(X_test, y_test, s=100, c=\"b\", edgecolor=\"k\", linewidths=1, label=\"True\")\n",
    "    axes[i].scatter(X_test, preds, s=100, c=\"r\", edgecolor=\"k\", linewidths=1, label=\"Predictions\")\n",
    "\n",
    "    # plot model\n",
    "    x = numpy.linspace(X.min(), X.max(), 500)\n",
    "    axes[i].plot(x, model.predict(x), \"-k\", alpha=0.8, linewidth=3.0, label=\"Model\")\n",
    "\n",
    "    axes[i].set_xlabel(\"X\")\n",
    "    axes[i].set_ylabel(\"y\")\n",
    "    axes[i].legend(loc=0)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_rdd = sc.textFile(\"file:///home/lsda/artificial_regression.csv\")\n",
    "points_rdd = points_rdd.repartition(4)\n",
    "print(\"Number of RDD partitions: {}\".format(points_rdd.getNumPartitions()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X_train)\n",
    "D = 1\n",
    "\n",
    "def read_points_batch(points):\n",
    "    \n",
    "    points = list(points)\n",
    "    \n",
    "    # first entry is label, the other ones the data point\n",
    "    arr = numpy.zeros((len(points), D + 1))\n",
    "    \n",
    "    # parse the strings to data points\n",
    "    for i, s in enumerate(points):\n",
    "        arr[i] = numpy.fromstring(s, dtype=numpy.float32, sep=',')\n",
    "    \n",
    "    return [arr]\n",
    "\n",
    "points_rdd = points_rdd.mapPartitions(read_points_batch)\n",
    "points_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLeastSquaresRegressionSpark():\n",
    "    \n",
    "    def __init__(self, lam=1.0, max_iter=10, random_state=0):\n",
    "        \"\"\" Instantiates the regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lam : float, default 1.0\n",
    "            The regularization parameter lambda\n",
    "        max_iter : int, default 10\n",
    "            Maximum number of iterations for optimizer\n",
    "        random_state : int, default 0\n",
    "            The random state (seed) to be used.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lam = lam\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Returns the parameters of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        return {\"lam\": self.lam, \n",
    "                \"max_iter\": self.max_iter,\n",
    "                \"random_state\": self.random_state}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        \"\"\" Sets the parameters of the model\n",
    "        \"\"\"        \n",
    "        \n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def fit(self, points, n, d):\n",
    "        \"\"\"\n",
    "        Fits the regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        points : RDD\n",
    "        n : Number of overall instances\n",
    "        d : Dimensionality of each feature\n",
    "        \"\"\"   \n",
    "    \n",
    "        from scipy.optimize import fmin_bfgs\n",
    "\n",
    "        # initial guess\n",
    "        w0 = numpy.zeros(d, dtype=numpy.float64)\n",
    "        \n",
    "        # additional parameters for the _function and _function_prime\n",
    "        args = (n, points, self.lam)\n",
    "\n",
    "        # Note that you do not necessarily have to provide fprime function, see\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html\n",
    "        # (in this case, the gradient will be approximated via the function, as explrained\n",
    "        # here: https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)\n",
    "        self._w = fmin_bfgs(self._function, w0, args=args, maxiter=self.max_iter)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _function(self, w, n, points, lam):\n",
    "        \n",
    "        # compute the function value based on the RDD in a \n",
    "        # distributed manner via 'map' and 'reduce'.\n",
    "        w = w.reshape((len(w),1))\n",
    "     \n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Computes predictions for a new set of points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "        \"\"\"           \n",
    "        \n",
    "        # make sure that we have numpy arrays; also\n",
    "        # reshape the array X to ensure that we have\n",
    "        # a multidimensional numpy array (ndarray)\n",
    "        X = numpy.array(X).reshape((X.shape[0], -1))\n",
    "            \n",
    "        preds = numpy.dot(X, self._w)\n",
    "                \n",
    "        return preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=1)\n",
    "\n",
    "lams = [10, 0.01, 0.0001]\n",
    "gammas = [0.1, 2, 1000]\n",
    "\n",
    "f, axes = plt.subplots(1, 3, sharey=True, figsize=(15,5))\n",
    "\n",
    "for (lam, gamma, i) in zip(lams, gammas, range(len(lams))):\n",
    "\n",
    "    model = LinearLeastSquaresRegressionSpark(lam=lam, max_iter=10)\n",
    "    model.fit(points_rdd, N, D)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # plot test points and predictions\n",
    "    axes[i].scatter(X_train, y_train, s=2, c=\"k\", edgecolor=\"k\", linewidths=1, label=\"Training\")\n",
    "    axes[i].scatter(X_test, y_test, s=100, c=\"b\", edgecolor=\"k\", linewidths=1, label=\"True\")\n",
    "    axes[i].scatter(X_test, preds, s=100, c=\"r\", edgecolor=\"k\", linewidths=1, label=\"Predictions\")\n",
    "\n",
    "    # plot model\n",
    "    x = numpy.linspace(X.min(), X.max(), 500)\n",
    "    axes[i].plot(x, model.predict(x), \"-k\", alpha=0.8, linewidth=3.0, label=\"Model\")\n",
    "\n",
    "    axes[i].set_xlabel(\"X\")\n",
    "    axes[i].set_ylabel(\"y\")\n",
    "    axes[i].legend(loc=0)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
